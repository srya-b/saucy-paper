Universal Composability is the leading framework for defining security of message-passing protocols between mutually distrustful parties \todo{better wording here to not say it's only cryptography and then talk about distributes systems}.
In recent years, blockchain protocols combine cryptographic and distributed protocols, and UC has become the preferred proving framework due to its compositional guarantees and modularity~\cite{blockchain, papers, that, do, this}
It's core feature allows complex protocols to rely on simpler building blocks, called ideal functionalities, that act as trusted third parties. 
The security definition ensures that protocols remain secure when the ideal functionalites they use are replaced by protocols that realize them.
Despite modularity being a key design principle, UC proofs and definitions, in literature, are often complex and difficult to undersatand, let alone reuse or falsify.
Furthermore, its complexity makes it difficult to penetrate by new users inundated with esoteric terminology and concepts, and ends up being used only as a reference for actual protocol implementations.

Existing tools apply programming language techniques to UC and attempt to mitigate these drawbacks~\cite{ipdl,easyuc,somemore} by introducing new languages to express UC defintiisons or proposing a formal logic to express UC security. 
Althought useful, such tools to little to make UC more approachable or usable by non-cryptography researchers because 
\begin{enumerate}
\item niche programming languages aren't suited to production environments and add to the already high bra for understanding UC
\item the added obligation of using language machinery or new tooling for mechanizes proofs on the programmer can be extensive~\cite{ironfleet}.
\end{enumerate}
This results in implementations that often depart from the UC model entirely, making existing proofs less useful .

\todo{somewhere between explain UC first?? but the issue there is we bury the lede (below) way too far into the paper}
In this work, we contend that the UC framework can be a practical tool for protocol implementations, especially byzantine protcols, rather than a purely theoretical tool.
Ideal functionalities behave like softare modules used during development because they concisely capture an interface, specification, and security guarantees.
Most importantly, they concretely define adversarial capability and the domain of adversarial actions expected. 
The UC execution model then is a clean and simple harness that allows for developing, testing, and composing together protocols under arbitrary adverasrial conditions.
Furthremore, maintaining a symmetry between paper-and-pencil definitions and the resulting implementation solidifies the validity of existing security proofs within the implementation.\todo{i want to say how the symmetry almost makes the code more valid/secure w.r.t. the proof}

We motivate our vision for UC through a realization of the framework in a mainstrain programming language: Haskell. 
This implementation realizes the ITM computation model, and provides type checking of channel interfaces between module though we envision more descriptive type systems can be applied to this task.
We push modular and programming-inspired UC designs further by providing a new abstraction for realizing asynchronous, and other arbitrary, networks that both greatly simplifies paper-and-pencil definitions/proofs and an \emph{asynchronous code} abstraction familiar to programmers.
Finally, we employ fuzz testing, a critically important and highly successful testing strategy in modern software engineering, to our own implementations of canonical and modern byzantine agreement protocols and showcase
\begin{itemize}
\item the UC framework especially lends itself to fuzz testing by reducing complex distributed systems to a set of simple protocol and adversarial interfaces that greatly reduce the input space to be searched
\item the real/ideal paradigm already provides a built-in specification, the ideal functionality, of the inteded protocol to test protocols properties against
\item our novel design of asynchronous computing/networking \todo{something something}
\end{itemize}

Rough notes for the paragrapph. 
We implement bracha, ben-or, aba and inject faults into them. show how simple fuzzers that don't target specific vulnerabilities can find bugs that violate agreement/safety/etf
the better approach might be to idenftify only the set of bugs that would induce failures in safety and then say that those can be identified
but what about simpler bugs? that would require more meaningful testing but not what UC is good fr
existing fuzz testing is already good for bugs in single compiled progra like a single protocol running in isolation, so we don't think of finding those
but finding those of a distributed nature


Rather than bridge the gap between cryptography reserachers and protocol implementers, these approaches aid validation but accept and embrace the complexity of the framework.
Ultimately, the advantages of the proving framework, and the paper proofs that rely on it, are lost because of code that completely departs from them. 




Universal Composability (UC)~\cite{canettiUC} is the leading framework for defining security properties of cryptographic protocols.
It is considered the strongest definitional model since it guarantees the security properties hold even when the protocol is arbitrarily composed with
multiple concurrently-executing sessions of other protocol.
UC has gained popularity for analyzing cryptographic protocols due to its \emph{ideal world/real world} simulation mechanism.
In contrast to game-based cryptography where security properties are defined via attack games,
UC defines the \emph{ideal functionality}, a trusted third party that serves as the \emph{protocol specification}.
A core feature of the frameowk is its modularity where complex protocols are defined in terms of simple, ideal functionality building blocks that are secure by definition. 
A major drawback of UC is that security proofs can be quite complicated and difficult to analyze. 
Many related works \cite{ilc, easyuc, ipdl, etc} attempt to formalize UC security through a new programming language or defining UC security in an existing formal verification lanuage, and,  
although useful, such tools frequently never make their way to software engineering practice because
1. niche programming languages aren't suited to large scale development and can be difficult to use, and 2. the added proof obligations on the programmer can be extensive~\cite{ironfleet}.
Utimately, UC-secure prtocols end up being implemented in software frameworks that do not replicate UC, and, therefore, my invalidate on-paper proofs without further security proofs of the code.

We address this gap in UC-driven software develoment by exploring how inforal security analysis of UC definitons, in our implementation of UC, can identify, and aid in eliminating, security vulnerabilities.
A key controbution of our software development framework for UC is proposing a noval new abstraction for capturing network assumptions. Our abstraction makes use of the novel import mechanism 
for polynomial time and fits nicely as a software abstraction. Our implementation and network model are, to the best of our knowledge, the first concretiziations of the import mechanism in this way.
Prior attempts at modelling asynchrnous networks, for example, focus primarily on adversarial delay of messages between parites. Such notions can require
protocols to encode signficant model-specific behavior which clutters functionalitiy (and protocol) definitions, places unecessary restrictions on protocol design, and is counter-productive for modular and reusable code. 
Out abstraction, on the other hand, acts are a wrapper around ITMs and offers a notion of \emph{asynchronous computation} in a way that is UC-compatible and firs well within a software framework. 
Not only is it natural for software development, but it also reduces functionality and protocol code to be almost model-agnostic. 
We use our network model of computation to focus solely on modeling and analzing distributed protocols in this work.
UC is most notably a framework for cryptographic protocols, however, in recent years the emergence of decentralized systems has renewed focus on modelling the security of asynchronous byzantine networks in UC~\cite{many,cit,ations}. 
Decentralized, namely blockchain, systems are highly modular with many protocols sharing state in unexpected ways and relying on numerous shared distrbuted sub protocols.
Naturally, compositional security in UC is ideal for capturing such protocols. 

We opt for fuzz testing, by generating environments, as our analysis tool for three important reasons. First, numerous prior work has demonstrated the success of fuzz testing at identifyin software bugs. 
Some work suggests fuzzing is comparable in success to even formal approach such as symbolic execution. 
Second, the UC framework lends itself to compact definitions that compose through ideal functionalities making the input space for protocol parties and adversaries much smaller. 
Combined with our simple network model, UC modularity makes it easier for generated environmenst to explore more of the state space for a particular protocol or simulator proof. \todo{is this setting us up with an obligation to prove this statement with some coverage testing?}
Third, the real/ideal paradigm, and the ideal functionality, provide a built-in specification against which protocols can be tested and a method for comparing the two (the UC experiment). \todo{this last one is the least good the point is that we don't have to define state machines and added spec on top of a protocol, they should already exist from on-paper definitions it isn't an additional obligation to the programmer when using haskell saucy fuzzing.}

Among the few related works that apply fuzzing to distributed systems, the work by Jepsen goes so far as to apply their methdology to a decentralized byzantine consensus protocol called Tendermint.
Jepsen deploys compiled Tenderming binaries and tests that operations on a distributed database are linearizable under various network conditions and limited byzantine behavior. 
As they admit in their results, the byzantine behavior that they capture is limited to replicated simple scenarios signing keys for multiple nodes. Designing a ful byzantine node for Tendermint is a considerable engineering effort.
Part of the hurdle is that testing a monolothic application like the Tendermint binary requires instrumenting and implementing every sub-component in the whole protocol.
Conversely, it is not possible, within \us, to test secrity under clock skews or race conditions in multi-threaded handling of network messages like Jepsen is able todo.
Hoever, this isn't a limitation of \us, but highlights a key distinction between us and works like Jepsen. \us is a \emph{development framework} rather than only a testing framework.
An application like Tendermint, implemented in \us, ensures that sub-components like network handling and clock timing are implemented and tested in isolation.
As mentioned in the previous paragraph, this makes allows us to test race conditions, should they arise, and capture byzantine complicated byzantine behaviort that Jepsen does not. 
\todo{feels like the point is there but not stated clearly enough. I'm trying to relate to the point of reduced input space from the previous paragraph to talk about why byzantine modelling is easy in \us and that not being able to do clock-skew type testing is a consequencer of Jepsen not being a development framework and having to work with existing monolithic binaries.}

\subsection{Below this is just notes}

\us is a not only a testing framework but a \emph{development framework} where subcomponents and sub protocols are created and tested in isolation.

Combined with paper and pencil proofs, we can reply on the composition theorem to test more complex protocols like Tendermint without worrying about the full code stack.
For example, even though \us can doesn't allow testing of race conditions resulting from multi-threaded handling of network messages 

our framework ensures that such sub-components of Tenderming are created and tested
within UC and composed correctly. 

Jepsen fuzz test a compiled binary for Tendermint and check that updates to a distributed database are linearizable under various network conditions and some byzantine behavior.



fuzz testing is a proven technique even when compared to formal analysis tool. this shows promise as a simple informal tool for protocol analysis
compared to things like jepsen running and testing protocols is much easier without any of the engineering effort require because of the subtle ways in which UC is designed to mimic a realistic computing environment and network
fuzzing byzantine messages is easier with UC for a few reasons:
* the simple interface for adversarial behavior makes it easy to play with message ordering and deliver / dropping messages
* the ideal functionality abstraction removes sub-protocol detail making the space of byzantine messgaes much smaller / manageable for fuzz testing

this leads into another point about comparing with jepsen. they test existing compiled binaries and so end up testing the full stack of code as part of their testing
they can test race conditions in how messages are received, for exampe, and standalone UC fuzz testing can not replicate that
but this is precisely where UC shines: not just testing code but developing it within the UC framework all such sub-protocols are tested and designed individually and larger, composed protocols only testing teir behavior with the ideal functionality model 



We address the state of software development around UC by exploring the extent to which informal security analysis of UC definitions, in our UC implementation can identify, and aid in elminiating, security vulnerabilities such as safety, correctness, and liveness in distributed systems .\todo{this first line needs to be better}.  
A key contribution of our software development framework around UC begins with proposing a novel abstraction for capturing different network models.
Prior attempts to capture, for example, asynchronous communication focus primarily on adversarial delay of messages between parties. 
Such notions can require protocols to encode significant model-specifi behavior in their definition which lutters definitions, places unecessary restrictions on protocol design, and is counter-productive for modular and reusable code. 
Our abstraction, on the other hand, acts as a wrapper around ITMs and extends the abstraction to asynchronous \emph{computation}: adverasrially delayed code rather than messages.  
Not only is this abstraction more natural for a software setting, but it also reduces functionalities and protocols to become almost model-agnostic in their definition. 
For asynchronous networks we dub our construction the \emph{asynchronous wrapper}. 
It uses the import mechanism introduced in UC to provide \emph{eventual} deliver guarantees.
To the best of our knowledge, it is the first concretization of the import mechanism with eventual delivery. 



\todo{a statement wrapping all this up in a key takeaway and positive result of this work in the context of the state of UC}.
