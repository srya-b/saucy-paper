In this section we dive into UC as a programming tool for distributed protocols.
We first make the case that the inherent features of UC, namely the real-ideal paradigm and the UC environment and adversarial modelling, are better for specifying, modelling, implementing, and testing distributed systems.
The added benefit of the implementation maintaining a close relationship to a powerful theoretical securuty proof system only bolsters UC's potential as a development environment.
We then go through out process of implementing these protocols in UC and highlight the restrictions and extra effort required to define a protocol in UC, but crucially highlight a clear correspondence between the effects of the UC style of programming and the design principles and constraints employed by man existing works that aim to improve distributed system design, implementation, and testing. 
This suggests that the extra work to realize a protocol around UC constraints follows well-cite principles of engineering better distributed systems.
Finally, we introduce fuzz testing to our implementation and apply it to finding bugs in three case studies from simple to complex byzantine protocols.

It is important to remind the reader that the implementations and testing done in this work are far from production-ready, and serve, instead to challenge the idea that UC is useful only for theoretical work and not useful as a practical development environment.
Furthermore, we hope that our work in this area should foster more research exploring, fine-tuning, and building better ways to utilize UC in for software development.


\subsection{The Real-Ideal Paradigm}
The primary feature of the UC framework is the real-ideal paradigm that can be used to prove security against a computationally bounded adversary.
Prior formalisms to UC used notions of comparison between idealized version of protocols, ideal functionalities, but UC was the first to constrain adversaries to being probilistic polynomial-time and to instroduce the PPT environment meant to capture protocol interaction with any other concurrently running protocol whose job it is to distinguish between the two worlds.
The ideal functionality as a specification of a protocol's properties, simulation-based against computationally bounded adverasries, and the environment's ability to control both protocol parties and the adversary make the paradigm a powerful and expressive framework for development and testing.

The ideal functionality as a specification allows for capturing arbitrary properties that are otherwise cumbersome and error-prone to state and prove in, say, a property-based definition.
This is especially true for protocols where different properties aren't easy isolated, or where notions of fairness or adversarial advantage must be specified, as we see in ABA.
Defining security of a protocol in relation to an idealized version that implicitly satisfies the desired through an idealized adversary is a simple and generalizable way to anayze the security of protocols.\todo{say something about the simulator being equivalent to an informal proof of security, it's designed to work for any possible adversary)}

\paragraph{Ideal Functionalities as Specifications}
Working with the idealize protocol and proving security in relation to it yields some critical benefits over property-based definitions.
We first give the canonical example of for illustrating the limitations of property-based definitions.
Consider a two-party computation where parties jointly compute a function on their inputs and desire \emph{correct} evaluation of the function and \emph{secrecy} where nothing other than the output of the function is revealed to either party.
Defining a good meaning of correctness is complicated by additional desirable properties like fairness where the adversary can not manipulate the output of the computation too much. 
Secrecy adds another layer into this if you take, for example, the function $f = x_1 \oplus x_2$ which xor's the inputs of the two parties. 
Secrecy in for this function is vacuously true because the either party can determine the other's input given just the output and their own input. 
\todo{finish this example from the previous work}

In general, the ideal functionality circumvents the need to explicitly state and define the desired properties of a protocol as a laundry list that can be difficult to specify. 
The ideal functionality avoids this pitfall by implicitly defining the desired properties, however cmplicated they may be, and proving security in relation to it avoids the cumbersome and error-prone nature of trying to analyze of prove properties individually~\cite{uc2006, definingmpc}.

Specifying adverasarial information that is leaked and defining distributions from which outputs or random choices may be made (and the adversary's influence over outcomes) makes the real-ideal paradigm particulary powerful.
Not only can we assert properties but we can additionally reason about properties like fairness or adversarial influence over computation outcomes.
Such a definition isn't easy to formulate as a standalone property in even the above 2-PC example where secrecy and correctness are intertwined with each other and with a notion of fairness/influence.

\paragraph{Modelling ABA}
In our implementation of the ABA protocol, for example, is was necessary to additionally define adversarial influence in our ideal functionality. 
We posit that defining software modules or packages in terms of ideal functionalities is more expressive than the current standard of property-based definitions as they capture more detailed and fine-grained information abaout the underlying protocol.

\rough{is it easier for others to test additional properties that may hold with stronger ideal functionalities for the same protocol? someone using ABA or Ben-Or might care about fairness or distribution of outputs for ABA given some input distributions?}

\paragraph{Simulation-based Testing}
\rough{The plan is to talk about defining security in terms of an ideal adversary for all other adversaries whose capabiities are concretely defined. Rather than asserting every individual property, indistinguishability becomes the single sole assertion that needs to be made.}

\plan{In addition to this, we point out that the combination of our async model and UC provides a framework with the strongest possible adversary that gets activated automatically between every activation of a protocol party and so has the most capability to force faults compared. Not neessarily an advantage but as mentioned this is a direct correspondence to additional work and automatng it rather than requiring environment/protocol parties to force it (like we mention in the wrapper section) permits an a simpler method of adversarial testing.}

\subsection{Relation To Existing Work}
Utilizing UC adds a fair bit of work to conform to the communication pattern it defines as well as write a proocol around the activation rules.
This extra effort is evident in our implementation of the ABA protocol and attepting handle arbitrary activation throughout different stages of the protocol and the straight-line execution on every activation.
In this section we briefly review existing literature whose aim it is to improve the quality of implementation and testing of distributed systems, and show a direct correspondence between the principles they propose/encourage/successfully employ and the side-effects of the UC style of programming.
We find that many of the UC principles result in code that minimizes concurrency, avoids a complicated locking structure, allows an adverasry to inject messages and execution between every activation of a protocol party, etc \todo{refer to the table from before}.





\plan{Add to this first paragraph: in this section we discuss UC and it's tradeoffs/advantages when designing protocols and we leave discussion for testing/verifying implementations for the fuzzing section}
UC is known to be a useful theoretical tool for defining distributed protocols, however crafting definitions in the UC style, whether on-paper or as an implementation, requires careful effort beyond what traditional definitions require.
Like many existing works that have successfully proposed and developed protocols by constraining programmer or protocol behavior~\cite{mace,macemc,farsite,farsiteretro,dbug}, UC imposes a set of rules that change how protocol designers have to think. 
The UC-style mimics maybe of the design principles highlighted by these workd, and, in some respects, takes them further to achieve the similar advantages. 
In this section we relate out experience of translating the ABA Protocol from \cite{ABA} and identify advantages, and some trade-offs, in expressing and implementing protocols in UC.
Namely, we connect the UC framework to existing goals such as modular design, minimization of concurrency hazards, and replayability/determinism of execution.
Furthermore, we note that forcing a programmer or designed to think and frame the problem through this framework forces more careful thought to how protocols may be activated or used incorrectly in a deployment setting---addressing some of the concerns raised by \cite{paxosthoughts} on the usefulness of theoretical model specs and pseudo-code.
Although other frameworks and designs, some of which we reference in this section, achieve similar goals, we argue that UC simulataneously achieves more of them than others, and, the existing gap between theory and practice demands it be taken seriously as a framework worth further exploring.

\textbf{REMEMBER}: An important thing to remember for this section is that we aren't trying to motivate using UC for distributed systems modelling on-paper.
Our point is that designing from a UC-first princiciples:
\begin{itemize} 
\item definitions and code are implemented in a framework that makes it easy to do theoretical work on and proving etc.. when it comes time for it in the future if you're starting with implementation
\item if starting with definitions, UC forces designers to be precise about network assmuptions, models, properties of primitives, adversarial capabilities concretely, how protocol is activated and what happens, initial conditions, etc...
\end{itemize}

% The UC-framework can be described as an event-driven framework with execution without preemption, and a penchant for buggery mr powers\todo{finish}.

% disction between changes required to pseudo code vs changes required in implementation
% in pseudocode you still need to be clear about activations and what to do in certain cases but can be vague about concurrency structure and differen "processes" talking to each other
% in code that has to be made explicit, but pseudocode that outlines how to handle events more specifically means implmenters aren't trying to themselves prove statements to determine what the protocol shoudl o
% for example, a developer aims to make the code as robust as possibleunder arbitrary conditions but the consequences of such choices may be unclear and actually undermine the intent of the paper definition. 
% TODO: the regarding the paxos paper comments on langauge specs or framework for specs are not usually useful in practice --> UC goes some of the way in forcing a designer to think about how to handle realistic scenarios of malformed input or unexpected activation by another process/ITM/message-passing functionality

\subsection{Design Constraints}
Our implementation of the UC framework, doesn't enforce a specific way to write code according to the UC rules and isn't a system that prevents writing ``bad'' code, like \cite{mace} or \cite{verdi}, but it is a set of principles that a programmer must adhere to to reap the benefits that we outline below.
% A similar method is used by Bolosky et al.~\cite{farsite} where the authors iterate through a few design principles and methodolodies to arrive at a way of writing distributed code that maximizes maintanability and modularty and minimizes concurrency hazards and non-determinism.
Of course, our UC implementation places some limitations on what the programmer does: enforcing message types for a protocol/functionality, forcing programmers to use the built-in typeclasses for environments/adversaries/protocols/functionalities, and limiting the ways in which these processes can communicate with each other.
%This approach is similar to the work by Bolosky et al.~\cite{farsite} who work through different iterations of a development framework and set of rules for organizing and writing distributed systems as event handlers. 
%The resulting framework from their efforts, and their ``pinning pattern'', bear a striking resemblance to the the communication rules that UC already implements. 
%Furthermore, their prioritization of preventing concurrency bugs and tackling source of non-determinism are shared by the UC semantics.
%Other works, such as Mace~\cite{mac} and dBug~\cite{dbug}, arrive at similar conclusions and try to instrument distributed code in a modular and layered way so that code is easy to maintain and parts are easy to replace or chop and change. 
%The ideal functionality model, in UC, takes this a step further by not only cleanly abstracting away functionality from lower layers but bridges a gap to theory where many protocols that can realize the same model and interface can be proven, both through theoretical simulation proofs and informal testing of simulators in our implementation, leading to more robust and modular code. 
%A clear example of this advantage arises in \cite{farsite} where nuanced and hard to track bugs arise from their assumptions on quick check implementations accross different operating systens.
%While both were correct, there lacked a clear model and set of properties in mind when implementing their protocol which led to an protocol implementation based on a specific instance of quickcheck rather.
%In the ideal functionality, model, the designer is forced to first undertake choosing an ideal functionality that \emph{succinctly captures the intended properties of a sorting algorith} and assert that the possible candidates that can replace the ideal functionality, when their code is deployed, satisfy some basic simulation properties with the ideal functionalitu or, in the best case, are proven to realize.

%In our experience translating the ABA protocol from ~\cite{aba}, we identify a few key takeaways and trade-offs with expressing protocols in this way. 
%Like existing work~\cite{farsite}, writing code in the UC-style and conforming to its execution rules is not enforced by some type system, but is instead a set of principles that programmers must adhere to.
%Notably, writing paper definitions in the UC styles has the advantage of bringing the paper and its proof closer to the end product: the implmentation.
%Thinking about a protocols as en \emph{event-driven} piece of code force the designed to be explicit about the precise conditions of a protocol party when a new messages is received. 
%Finally, the \emph{write-after-read} semantics of UC ensure that on every event, there is a straight-line and deterministic execution which eliminates a large degree of concurrency hazards that arise in traditional programming. 
%This is a dominant concern, motivation, and goal for existing works that propose frameworks for writing and/or testing code~\cite{farsite, mace, macmc, dbug}, and UC largely addresses most of them already.
%A notable drawback, also pointed out in prior work, is that cosntraints around atomicity of action or a layered approach to programming (like UC's ideal functionality model) stand in the way of high-performance code.
%Though true, we remark that UC is as yet unexplored as a framework for development, and we are only making its case as a candidate for further study in this new domain---challenging the convention wisdom around it.

\paragraph{Modelling Assumptions} 
% POINTS: asynchronous/network assumptions ; assumptions about primitives by using ideal functionalities 
A key advantage of the UC model is that it forces a designer to model both their protocol and the assumed primitives it takes advantage of.
A simple model of the Bracha protocol has to first specify and ideal functionality that exactly captures the intended properties of broadcast.
The model-first approach to system design forces the expected safety, validity, reliability, or timing properties to be clear from the outset.
The Bracha protocol itself exists in a world with a hybrid functionality that described the intended properties expected of the primitives that it uses.
For Bracha broadcast this is a functionality that captures the precise definition of the \emph{asynchronous network} that the protocol assumes it is in.
Not only are timing guaranteed captured, but the ideal functionality defines the adversary's capabilities to delay, reorder, or even modify messages. 

The necessity for designing protocols in a functionality-first way is, first, critical to first ensure systems are not tied to specific implementations~\cite{farsiteretro,farsite} that may change in a different environment (or operating system, for example).
Second, being clear about assumptions, such as precise network modelling, ensures the gap between design/theory and implemenation is minimized. 
In our experience translating the ABA~\cite{ABA} protocol into UC, a big step was ensuring that the minimal asynchronous network asumptions that we implemented the protocol nuder continued to satisfy the intended properties, and there weren't unstated assumptions such as ``all messages in round $r$ are received before round $r+1$''.
Third, we envision a future where there is a large corpus of ideal functionalities and protocols realizing them---proven secure both on paper and in implementations---that programmers can easily plug-in to build larger systems and fall back to UC's compositional security.
% point: you almost end up trying to make the protocol more robust by looking at messages before input is available and you add all these extra steps to the design that are untested or unexplored on paper and you run the risk of departing from the intended behavior. With UC on paper and UC impleenetation and design the gap between the two is much smaller 
\plan{There's something to say about existing works modelling systems in higher-level specs which can be analogous to ideal functionalities}

\todo{talk about some future things in the context of related-works for model specificatios, model checking, etc...}

\paragraph{Realism? (need better heading)}
% POINTS: designs should force encapsulation of spurious events like activation before input ; reduce the gap between specs/definitions/paper and implementation ; less additional work required by programmers ; highlight the benefir in both directions theory <-> implementation
Using UC as a starting and ending point for engineering distributed protocols also overcomes some of frequently mentioned limitations of pseudo-code or on-paper specifications~\cite{paxosthoughts}.
A criticism of such definitions can be that they don't take into account what a program might actuall encounter in the real world.
For example, in the real world programs may be used incorrectly by users or a node's router may temporarily go offline or the program in an agreement protocol might receive messages before it even has a value to propose. 


\plan{Ideal functionalities can't capture any and all properties a designed might want to specify: aba has a property that if all parties have same input then they decide by round r+1. The ideal functionality on its own can't capture timing guarantees like that, so that's a downside but it is still something checkable.}
\todo{Should this point be included in this section, the fuzzing section, or both?}


\paragraph{drafting notes}
There is a statement in the Paxos implementation retrospective that states that often times paper specs aren't very useful for implementation since implementation has to consider so many more things
we state that UC formulation gets you a lot closer by having to be explicit about things like initial conditions, arbitrary message ordering and delivery
UC is useful for theoretical results, but for these reasons that make it good for testing code as well as writing code
it's evidenced by these related works that all attempt to do things very reminiscent of the UC framework but in an ad-hoc way
rather we say that we should consider UC as a candidate for development frameworks rather than them because it achieves largely the same things and supports the same concurrency protection and layering of code and the ideal functionality model provides all these things


\subsection{Implementation Constraints}
Enforcing design and implementation constraints on programmers and designers has tradeoff's as well. 
As referenced by existing work, forcing the programmer into a consrained set of possible designs requires more work and can be cumbersome as well.
Even so, we believe that the advantages of the UC style of programming outweight the disadvanages
Not all the disadvantages are inherent and some may be overcome in future with more focus on this area of research. 



% making explicit the network assumptions in an ideal functionality
%     capturing the properties expected, explicitly
% adding additional handling for initial conditions if activated by fMulticast first
% event-driven programming and read-after-write rules
%     straight-line execution
%     deterministic code path
%     one process activates one-other process
%     "environment activated in betwee" is related to the fuzzing section
  
